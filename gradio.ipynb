{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a317918e-a370-44a6-9cba-25a5385fb1ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://0.0.0.0:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://localhost:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "import torch\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    TaskType,\n",
    ")\n",
    "\n",
    "import gradio as gr\n",
    "from gradio import components as gc\n",
    "import os\n",
    "import joblib\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "\n",
    ")\n",
    "\n",
    "def main():\n",
    "    # Define a custom theme with a red primary button\n",
    "    custom_theme = gr.themes.Glass(primary_hue=\"blue\")\n",
    "\n",
    "    def predict_with_bert_base(driver_input):\n",
    "        # Load the LabelEncoder object from the file\n",
    "        label_encoder_dir = '../label-encoder'\n",
    "        label_encoder_file_path = os.path.join(label_encoder_dir, 'label_encoder.joblib')\n",
    "        loaded_le = joblib.load(label_encoder_file_path)\n",
    "        \n",
    "        # Load your trained model and tokenizer\n",
    "        model_path = \"../bert-models/best_model\"\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\") \n",
    "        model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "        \n",
    "\n",
    "        \n",
    "        inputs = tokenizer(driver_input, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        \n",
    "        # Get predicted label\n",
    "        predicted_index = torch.argmax(outputs.logits, dim=1).item()\n",
    "        confidence = torch.nn.functional.softmax(outputs.logits, dim=1)[0][predicted_index].item()\n",
    "        original_text_label = loaded_le.inverse_transform([predicted_index])[0]\n",
    "        return original_text_label\n",
    "\n",
    "    def predict_with_qlora(driver_input):\n",
    "        label_encoder_dir = '../label-encoder'\n",
    "        label_encoder_file_path = os.path.join(label_encoder_dir, 'label_encoder.joblib')\n",
    "        loaded_le = joblib.load(label_encoder_file_path)\n",
    "        \n",
    "        # Specify the path where your model was saved\n",
    "        qlora_path = \"../qlora-models/best_model\"\n",
    "        model_path = \"bert-large-uncased\"\n",
    "        \n",
    "        # Specify the quantization and LoRA configurations as used during training\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16\n",
    "        )\n",
    "        \n",
    "        peft_config = LoraConfig(task_type=TaskType.SEQ_CLS, inference_mode=False, r=12, lora_alpha=32, lora_dropout=0.1)\n",
    "        \n",
    "        model = AutoModelForSequenceClassification.from_pretrained(model_path,num_labels=5, quantization_config=bnb_config)\n",
    "        \n",
    "        model.load_adapter(qlora_path)\n",
    "        \n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_path) \n",
    "        \n",
    "        inputs = tokenizer(driver_input, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        \n",
    "        # Get predicted label\n",
    "        predicted_index = torch.argmax(outputs.logits, dim=1).item()\n",
    "        #confidence = torch.nn.functional.softmax(outputs.logits, dim=1)[0][predicted_index].item()\n",
    "        original_text_label = loaded_le.inverse_transform([predicted_index])[0]\n",
    "        return original_text_label     \n",
    "\n",
    "    # Define the function for the Interface\n",
    "    def driver_prediction(driver_input, model_choice):\n",
    "        if model_choice==\"bert-base-uncased\":\n",
    "            output = predict_with_bert_base(driver_input)\n",
    "        if model_choice==\"bert-large-uncased (loaded with qlora)\":\n",
    "            output = predict_with_qlora(driver_input)\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "    with gr.Blocks(theme=custom_theme, title=\"Driver Intent Predictor\") as demo:\n",
    "        gr.Markdown(\"# Driver Intent Classifier\")\n",
    "        gr.Markdown(\"This application will classify the driver's intent based upon the models that have been trained.\")\n",
    "        gr.Markdown(\"Two models are implemented here. The first model was trained using the classic LLM fine-tuning pipeline. The second model was trained using QLoRA.\")\n",
    "        with gr.Row():\n",
    "            driver_input = gc.Textbox(label=\"Driver Input\", placeholder=\"Enter the driver's input statement\")\n",
    "        with gr.Row():\n",
    "            model_choice = gc.Dropdown(label=\"Choose Trained Model\", choices=[\"bert-base-uncased\", \"bert-large-uncased (loaded with qlora)\"])\n",
    "\n",
    "        btn = gc.Button(\"Submit\", elem_id=\"custom_submit_btn\")\n",
    "        output_txt = gc.Textbox(label=\"Driver Intent\")\n",
    "        btn.click(driver_prediction, inputs=[driver_input, model_choice], outputs=[output_txt])\n",
    "\n",
    "    demo.launch(server_port=7860,server_name=\"0.0.0.0\")\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1d1107-a8da-4baa-858d-34ffd00788ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
