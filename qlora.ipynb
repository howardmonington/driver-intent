{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "341cdf6c-0067-4962-a68d-86a098d32c70",
   "metadata": {},
   "source": [
    "# Fine-Tuning BERT for Classification\n",
    "In this notebook, I fine-tune an encoder-only model for classification. I'm doing this so that I can show the differences in the classic fine-tunine pipeline compared to the QLoRA pipeline. The QLoRA pipeline has numerous differences. This classic pipeline works very well, but it's main disadvantage is that it can require a lot of GPU VRAM. I am currently doing this project using an RTX 4090 GPU (laptop edition), which has 16 GB of VRAM. This amount of VRAM is enough for bert-base-uncased, which has about 110M parameters, but it is not enough VRAM for bert-large-uncased, which has about 336M parameters. Therefore, if I want to load a larger model into the GPU VRAM for fine-tuning or inferencing, I would not be able to load it in the traditional way.\n",
    "\n",
    "## Additional Resources on QLoRA\n",
    "\n",
    "For a more in-depth understanding and practical insights on QLoRA, I created the following resources:\n",
    "\n",
    "- I made a detailed video explanation about QLoRA, providing a deeper understanding of the concept. Watch the video [here](https://www.youtube.com/watch?v=n90tKMDQUaY&t=1s&ab_channel=LukeMonington).\n",
    "  \n",
    "- I wrote an extensive article offering a theoretical perspective along with practical considerations. Read the article [here](https://medium.com/@lukemoningtonAI/fine-tuning-llms-in-4-bit-with-qlora-2982cddcd459).\n",
    "  \n",
    "- I made a tutorial video on implementing QLoRA in code, which can be found [here](https://www.youtube.com/watch?v=2bkrL2ZcOiM&ab_channel=LukeMonington).\n",
    "\n",
    "## High Level Overview\n",
    "In the traditional method, fine-tuning a large AI model required hefty computational power and expensive GPU resources, making it inaccessible to the open-source community. The model would be fine tuned on a larger data type, and then it was necessary to undertake a process known as 4-bit quantization to run the model on a consumer-grade GPU post-fine-tuning . This process optimized the model to use fewer resources but sacrificed the full power of the model, diminishing the overall results.\n",
    "\n",
    "QLoRA addresses this predicament, offering a win-win scenario. This optimized method allows for fine-tuning of large LLMs using just a single GPU while maintaining the high performance of a full 16-bit model in 4-bit quantization. With QLoRA, the barrier to entry for fine-tuning larger, more sophisticated models has been significantly lowered. This broadens the scope of projects that the open-source community can undertake, fostering innovation and facilitating the creation of more efficient and powerful applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6245de51-7d42-492a-913d-9697d045a84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some useful links:\n",
    "# https://huggingface.co/docs/peft/quicktour\n",
    "# https://huggingface.co/docs/peft/conceptual_guides/lora\n",
    "# https://blog.ovhcloud.com/fine-tuning-llama-2-models-using-a-single-gpu-qlora-and-ai-notebooks/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd501a24-02de-492d-be73-008a910832f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import torch\n",
    "import wandb\n",
    "\n",
    "import bitsandbytes as bnb\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from datasets import load_from_disk\n",
    "\n",
    "from peft import (\n",
    "    prepare_model_for_kbit_training,\n",
    "    LoraConfig,\n",
    "    TaskType,\n",
    "    get_peft_model\n",
    ")\n",
    "\n",
    "from transformers import (\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    AutoModelForSequenceClassification,\n",
    "    BitsAndBytesConfig,\n",
    "    EarlyStoppingCallback\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f760fde0-2333-468d-a312-8a33f0a8de30",
   "metadata": {},
   "source": [
    "### Initializing Weights & Biases.\n",
    "\n",
    "In order to track my runs, I am using Weights & Biases. There are other tools that can also be useful for this, such as TensorBoard or Aim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5190276a-5844-4ed5-9e72-1b3a74630bee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlukemonington3\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/workspace/notebooks/driver-intent/wandb/run-20231001_220623-wk0nenr3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/lukemonington3/driver-intent-classification/runs/wk0nenr3' target=\"_blank\">qlora-run</a></strong> to <a href='https://wandb.ai/lukemonington3/driver-intent-classification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/lukemonington3/driver-intent-classification' target=\"_blank\">https://wandb.ai/lukemonington3/driver-intent-classification</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/lukemonington3/driver-intent-classification/runs/wk0nenr3' target=\"_blank\">https://wandb.ai/lukemonington3/driver-intent-classification/runs/wk0nenr3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/lukemonington3/driver-intent-classification/runs/wk0nenr3?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f529848a770>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(project=\"driver-intent-classification\", name=\"qlora-run\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed76d191-a169-4e13-8a77-a12a20cf9484",
   "metadata": {},
   "source": [
    "### Loading the Dataset\n",
    "I'm going to be loading the same dataset that I used for the bert model. This is a synthetic dataset that I created myself for this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa8e3a76-30b7-46fe-8a0c-eee7b05eb3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_retrieve = \"../tokenized_dataset\"\n",
    "dataset_dict = load_from_disk(path_to_retrieve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba85b9eb-137c-49b4-a676-74e8d0b6086a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param:.2f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2ba156-6aef-46e2-b3c5-7237f4348540",
   "metadata": {},
   "source": [
    "# Implementing 4-Bit Quantization\n",
    "\n",
    "For this project, I am going to be fine-tuning a model called `bert-large-uncased`. This is a model contains about 336M parameters. If I tried to load the entire model onto my RTX 4090 laptop edition, it would cause me to get an Out of Memory (OOM) error. The model is simply too large to fit onto a GPU with only 16 GB of VRAM. This is where QLoRA Comes in.\n",
    "\n",
    "Below, I am implementing 2 innovations from QLoRA.\n",
    "\n",
    "1) First I am using the 4-bit NormalFloat data type, which is a novel approach in quantization that’s engineered for normally distributed data. This data type exhibits better empirical performance compared to the existing 4-bit integers and 4-bit floats, optimizing the storage and processing of data.\n",
    "2) The second innovation is the introduction of Double Quantization. This technique essentially quantizes the quantization constants, leading to a substantial reduction in the memory footprint. Specifically, it saves an average of about 0.37 bits per parameter.\n",
    "\n",
    "### What is Quantization?\n",
    "Quantization is a process that involves reducing the amount of data required to represent an input. This is typically achieved by converting a data type with a high bit count into a data type with a lower bit count. For instance, a 32-bit floating point number might be converted into an 8-bit integer. This process helps manage the range of the lower bit data type more effectively by rescaling the input data type into the target data type’s range using normalization.\n",
    "\n",
    "To illustrate, suppose we’re looking to quantize a tensor from 32-bit floating point (FP32) to an 8-bit integer (Int8) which has a range of [-127, 127]. The conversion would use a quantization scale, or constant, to accomplish this. Dequantization is simply the reversal of this process, turning the quantized data back into its original form.\n",
    "\n",
    "\n",
    "### How Can We Still Achieve the Same Level of Performance with a Smaller Data Type?\n",
    "The idea is that there are two data types: a storage data type (usually a 4-bit NormalFloat) and a computation data type (a 16-bit BrainFloat).\n",
    "\n",
    "The storage data type is where data is kept when it’s not being used (while still on the GPU). It’s been simplified and uses less memory, but it’s not quite ready for use yet. The computation data type, on the other hand, is the data in its ready-to-use state. When the data is needed for a task (the forward and backward pass), it’s converted from the storage data type to the computation data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "834a7a63-e513-44bb-a10a-dbbd3d1e0fdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 31886341 || all params: 183627781 || trainable%: 17.36\n"
     ]
    }
   ],
   "source": [
    "model_id = \"bert-large-uncased\"\n",
    "num_labels=5\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_id, num_labels=5,quantization_config=bnb_config, device_map={\"\":0})\n",
    "\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf70859b-cac9-4318-ba3c-c2965e0340c7",
   "metadata": {},
   "source": [
    "# Implementing LoRA\n",
    "After I have loaded the model in its quantized state, my next step is to implement LoRA. \n",
    "\n",
    "The concept of Low-Rank Adaptation, or LoRA, is a significant contribution to the field of Natural Language Processing (NLP). It’s a technique particularly useful when working with large-scale language models, like GPT-3 175B. The standard practice is to pre-train these models on a general domain and then fine-tune them for specific tasks or domains. However, as the models get larger, fine-tuning every parameter becomes less practical and more resource-intensive.\n",
    "\n",
    "This is where LoRA comes in. Instead of fine-tuning all the parameters, LoRA keeps the pre-trained model weights frozen and introduces trainable rank decomposition matrices into each layer of the Transformer architecture, the underlying architecture of models like GPT and BERT. This substantially reduces the number of parameters that need to be trained for downstream tasks.\n",
    "\n",
    "### How Much Space can LoRA Save?\n",
    "To put it in perspective, LoRA can decrease the number of trainable parameters by up to 10,000 times and reduce GPU memory requirements by 3 times, compared to the conventional fine-tuning approach. What’s more impressive is that it achieves this while maintaining or even surpassing the model performance quality.\n",
    "\n",
    "Taking this a step further, LoRA uses a small set of trainable parameters, often referred to as adapters, while the main model parameters remain fixed. The gradients generated during the training phase are channeled through the fixed pre-trained model weights to the adapter. The adapter, in turn, gets optimized to improve the loss function, enhancing the model’s performance on the task at hand.\n",
    "\n",
    "LoRA also adds a twist to this process by introducing an additional factorized projection to the existing linear projection. This creates a new component in the projection equation that is highly adaptable to the task at hand, providing further efficiency in the fine-tuning process. Thus, LoRA is a promising approach when it comes to refining large language models for specific tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39a597cf-95d7-4f2f-93e3-cd2fdcc07a7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,189,898 || all params: 336,331,786 || trainable%: 0.35378695964228607\n"
     ]
    }
   ],
   "source": [
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "def find_all_linear_names(model):\n",
    "    cls = bnb.nn.Linear4bit #if args.bits == 4 else (bnb.nn.Linear8bitLt if args.bits == 8 else torch.nn.Linear)\n",
    "    lora_module_names = set()\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, cls):\n",
    "            names = name.split('.')\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "\n",
    "    if 'lm_head' in lora_module_names:  # needed for 16-bit\n",
    "        lora_module_names.remove('lm_head')\n",
    "    return list(lora_module_names)\n",
    "\n",
    "modules = find_all_linear_names(model)\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=64,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"all\",\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    target_modules = modules,\n",
    "    # when we wrap our base model with PeftModel and pass the configuration, we obtain a new model in which only the LoRA \n",
    "    # parameters are trainable, while the pre-trained parameters and the randomly initialized classifier parameters are kept\n",
    "    # frozen. However, we do want to train the classifier parameters. By specifying the modules_to_save argument, we ensure \n",
    "    # that the classifier parameters are also trainable, and they will be serialized alongside the LoRA trainable parameters \n",
    "    # when we use utility functions like save_pretrained() and push_to_hub().\n",
    "    modules_to_save=[\"decode_head\"],\n",
    ")\n",
    "\n",
    "peft_config = LoraConfig(task_type=TaskType.SEQ_CLS, inference_mode=False, r=12, lora_alpha=32, lora_dropout=0.1)\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb3dd0e-1edf-4b35-8508-f3eff2c84658",
   "metadata": {},
   "source": [
    "# Evaluation Metrics for Multi-class Classification\n",
    "\n",
    "In multi-class classification tasks, it is crucial to choose the right metric to evaluate the performance of the model. Several metrics could be considered, each with its own advantages and disadvantages depending on the specific task and the dataset. Below are some of the commonly used metrics for multi-class classification:\n",
    "\n",
    "1. **Accuracy**: \n",
    "    - Accuracy is the ratio of correctly predicted instances to the total instances. It's a straightforward metric to understand and works well when the classes are balanced and the cost of misclassification is the same across different classes.\n",
    "    \\[ \\text{Accuracy} = \\frac{\\text{Number of Correct Predictions}}{\\text{Total Number of Predictions}} \\]\n",
    "\n",
    "2. **Precision, Recall, and F1-Score**:\n",
    "    - Precision is the ratio of correctly predicted positive observations to the total predicted positives. \n",
    "    - Recall (Sensitivity) - the ratio of correctly predicted positive observations to the all observations in actual class.\n",
    "    - The F1 Score is the weighted average of Precision and Recall. It tries to find the balance between precision and recall.\n",
    "    - These metrics are useful when the costs of false positives and false negatives are significantly different.\n",
    "\n",
    "3. **Confusion Matrix**:\n",
    "    - A confusion matrix is a table that is often used to describe the performance of a classification model on a set of data for which the true values are known. It gives a more detailed view of what kind of errors the model is making.\n",
    "\n",
    "4. **Macro and Micro Averaging**:\n",
    "    - In a multi-class classification setup, precision, recall, and F1-score can be calculated on a per-class basis, but summarizing them into a single figure requires either macro-averaging (calculate metric for each class independently and then average) or micro-averaging (aggregate the contributions of all classes to compute the average metric).\n",
    "\n",
    "5. **Log-Loss**:\n",
    "    - Log Loss is the loss function used in (multinomial) logistic regression and extensions of it such as neural networks. It's a measure of error and unlike accuracy, the log loss metric is more sensitive to the confidence of the predictions.\n",
    "\n",
    "In the context of this project, I am tackling a driver intent classification problem where the task is to predict the driver's intent among five classes: lowering the driver side window, lowering the passenger side window, turning on the A/C, and others. Given that I created a balanced dataset (each class is equally represented), using **accuracy** as my metric is a sensible choice. It provides a clear and understandable measure of our model's performance across all classes, and the cost of misclassification is assumed to be the same across different classes. Further, since I have balanced classes, I do not need to worry about the model biasing towards the majority class, which can sometimes happen in imbalanced settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3df9e337-1e9f-4b92-b0da-11e2715d49b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(p):\n",
    "    logits, labels = p.predictions, p.label_ids\n",
    "    preds = logits.argmax(axis=-1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    wandb.log({\"accuracy\": acc})  \n",
    "    return {\"accuracy\": acc}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844218d5-81c6-4426-9222-acfa67f0177c",
   "metadata": {},
   "source": [
    "# Callbacks and Their Utility\n",
    "\n",
    "Callbacks in machine learning are a type of function that can be applied at certain stages of training processes, allowing more control during training, monitoring, or even altering the behavior of the model during training. They can help to get insights into the internal states of the model during training, perform actions, log information, or even stop training early if a certain condition is met.\n",
    "\n",
    "In this case, I utilized a callback from the Transformers library called `EarlyStoppingCallback`. This particular callback helps to stop the training process once the model ceases to improve, saving computational resources and time. It's particularly beneficial in preventing overfitting, where the model starts learning the noise in the training data rather than the actual underlying pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5ba5330-752e-467a-962d-747b06b558bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_callback = EarlyStoppingCallback(early_stopping_patience=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4149dc16-597e-4a26-83ef-6a8221498d4c",
   "metadata": {},
   "source": [
    "# Fine-Tuning for Multi-Class Classification\n",
    "\n",
    "Fine-tuning involves making minimal adjustments to a pre-trained model to adapt it to new, but related data. In the context of multi-class classification for driver intent recognition, fine-tuning a pre-trained model like BERT can provide a strong foundation of language understanding while adapting to the specifics of the task at hand. This not only saves computational resources compared to training a model from scratch but often achieves higher performance.\n",
    "\n",
    "* `per_device_train_batch_size` and `per_device_eval_batch_size`: These parameters control the batch size during training and evaluation, respectively, which affects the memory usage and potentially the performance of the model. \n",
    "* `num_train_epochs`: Specifies the number of times the entire training dataset will be passed through the model.\n",
    "* `evaluation_strategy`, `save_steps`, and `save_total_limit`: These parameters control the evaluation and saving of the model during training, enabling efficient monitoring and ensuring that only a specified number of model checkpoints are saved to conserve storage space.\n",
    "* `load_best_model_at_end`: Ensures that the best model is loaded at the end of training for further use or analysis.\n",
    "* `learning_rate`: Controls the step size at each iteration while moving towards a minimum of the loss function, a crucial parameter for the convergence and the performance of the trained model.\n",
    "* `metric_for_best_model`: Specifies the metric to use for model evaluation, in this case, accuracy, which is a suitable choice given the balanced nature of the dataset.\n",
    "\n",
    "The Trainer class from the Transformers library encapsulates the training process, providing a simple and efficient way to train and evaluate the model on the specified datasets. Additionally, the callbacks parameter allows the inclusion of previously discussed EarlyStoppingCallback, optimizing the training process by stopping it once the model ceases to improve.\n",
    "\n",
    "### Implementation of Third Concept from QLoRA Paper\n",
    "The third advancement from QLoRA pertains to the use of Paged Optimizers, which employs NVIDIA’s unified memory to mitigate the memory spikes during the processing of mini-batches with extended sequence lengths. This technique helps avoid the gradient checkpointing memory overload, which could potentially hinder the smooth processing of data.\n",
    "\n",
    "Paged Optimizers involves grasping the functionality of the NVIDIA unified memory feature. This tool behaves as a mechanism to control memory traffic. In instances where a GPU is reaching its memory capacity during data processing, this feature intervenes. It automatically transfers data between the CPU and GPU, effectively averting memory-related issues. This mechanism resembles the process in which a computer shuffles data between its RAM and disk storage when facing low memory scenarios. Paged Optimizers harness this feature. So, when GPU memory reaches its limit, optimizer states are temporarily relocated to CPU RAM, freeing up space on the GPU. These states are then reloaded back into GPU memory as needed in the optimizer update step.\n",
    "\n",
    "Here, I implemented this concept by utilizing the `paged_adamw_8bit` optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "421aa76e-4e42-4728-9b88-4d149fd181e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='96' max='96' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [96/96 11:29, Epoch 7/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.614400</td>\n",
       "      <td>1.463681</td>\n",
       "      <td>0.420000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.319400</td>\n",
       "      <td>0.984158</td>\n",
       "      <td>0.585000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.832900</td>\n",
       "      <td>0.559659</td>\n",
       "      <td>0.795000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.529400</td>\n",
       "      <td>0.387306</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.359500</td>\n",
       "      <td>0.243982</td>\n",
       "      <td>0.920000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.188700</td>\n",
       "      <td>0.115727</td>\n",
       "      <td>0.970000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.098700</td>\n",
       "      <td>0.055973</td>\n",
       "      <td>0.995000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.070900</td>\n",
       "      <td>0.046126</td>\n",
       "      <td>0.990000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.054600</td>\n",
       "      <td>0.029868</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=96, training_loss=0.5300428237145146, metrics={'train_runtime': 695.7645, 'train_samples_per_second': 9.199, 'train_steps_per_second': 0.138, 'total_flos': 2888376432721920.0, 'train_loss': 0.5300428237145146, 'epoch': 7.68})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=8,\n",
    "    output_dir='../qlora-models',\n",
    "    num_train_epochs=8, # 8\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_steps=10,\n",
    "    save_total_limit=5,\n",
    "    remove_unused_columns=False,\n",
    "    run_name='run_name',\n",
    "    logging_dir='/logs',\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    learning_rate=5e-4,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    report_to='wandb', \n",
    "    metric_for_best_model=\"accuracy\", \n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset_dict[\"train\"],\n",
    "    eval_dataset=dataset_dict[\"test\"],\n",
    "    compute_metrics=compute_metrics, \n",
    "    callbacks=[early_stopping_callback],\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65cee6b-5ad5-4f35-b00e-0fb77472f622",
   "metadata": {},
   "source": [
    "# Saving the Model\n",
    "Once the model finishes training, the berst model is loaded. I save this model separately so that I can use it later in the Gradio application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93922c1b-7b5f-4924-9e3a-c3e719ada75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"../qlora-models/best_model\"\n",
    "model.save_pretrained(model_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
